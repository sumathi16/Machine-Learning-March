{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>Gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race   Gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data =pd.read_csv(\"data/Adult/adult.data\",names = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\\\n",
    "            'marital_status',\"occupation\",\"relationship\",\"race\",\"Gender\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\\\n",
    "                                             \"native-country\",\"income\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">50K, <=50K.\n",
    "age: continuous.\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "fnlwgt: continuous.\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "education-num: continuous.\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "sex: Female, Male.\n",
    "capital-gain: continuous.\n",
    "capital-loss: continuous.\n",
    "hours-per-week: continuous.\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " <=50K    24720\n",
       " >50K      7841\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.income.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " <=50K    0.75919\n",
       " >50K     0.24081\n",
       "Name: income, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.income.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education_num     0\n",
       "marital_status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "Gender            0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.076646</td>\n",
       "      <td>0.036527</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.057775</td>\n",
       "      <td>0.068756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>-0.076646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043195</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.010252</td>\n",
       "      <td>-0.018768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>education_num</td>\n",
       "      <td>0.036527</td>\n",
       "      <td>-0.043195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122630</td>\n",
       "      <td>0.079923</td>\n",
       "      <td>0.148123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital-gain</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.122630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031615</td>\n",
       "      <td>0.078409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>capital-loss</td>\n",
       "      <td>0.057775</td>\n",
       "      <td>-0.010252</td>\n",
       "      <td>0.079923</td>\n",
       "      <td>-0.031615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>0.068756</td>\n",
       "      <td>-0.018768</td>\n",
       "      <td>0.148123</td>\n",
       "      <td>0.078409</td>\n",
       "      <td>0.054256</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age    fnlwgt  education_num  capital-gain  capital-loss  \\\n",
       "age             1.000000 -0.076646       0.036527      0.077674      0.057775   \n",
       "fnlwgt         -0.076646  1.000000      -0.043195      0.000432     -0.010252   \n",
       "education_num   0.036527 -0.043195       1.000000      0.122630      0.079923   \n",
       "capital-gain    0.077674  0.000432       0.122630      1.000000     -0.031615   \n",
       "capital-loss    0.057775 -0.010252       0.079923     -0.031615      1.000000   \n",
       "hours-per-week  0.068756 -0.018768       0.148123      0.078409      0.054256   \n",
       "\n",
       "                hours-per-week  \n",
       "age                   0.068756  \n",
       "fnlwgt               -0.018768  \n",
       "education_num         0.148123  \n",
       "capital-gain          0.078409  \n",
       "capital-loss          0.054256  \n",
       "hours-per-week        1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education_num  capital-gain  capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22cd3a878c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "data[\"capital-gain\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22cd3e1cac8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log1p(data[\"capital-gain\"]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29849"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['capital-gain']==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32556</td>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32557</td>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32558</td>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32559</td>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32560</td>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education_num  capital-gain  capital-loss  hours-per-week\n",
       "0       39   77516             13          2174             0              40\n",
       "1       50   83311             13             0             0              13\n",
       "2       38  215646              9             0             0              40\n",
       "3       53  234721              7             0             0              40\n",
       "4       28  338409             13             0             0              40\n",
       "...    ...     ...            ...           ...           ...             ...\n",
       "32556   27  257302             12             0             0              38\n",
       "32557   40  154374              9             0             0              40\n",
       "32558   58  151910              9             0             0              40\n",
       "32559   22  201490              9             0             0              20\n",
       "32560   52  287927              9         15024             0              40\n",
       "\n",
       "[32561 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select_dtypes(exclude=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method select_dtypes in module pandas.core.frame:\n",
      "\n",
      "select_dtypes(include=None, exclude=None) method of pandas.core.frame.DataFrame instance\n",
      "    Return a subset of the DataFrame's columns based on the column dtypes.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    include, exclude : scalar or list-like\n",
      "        A selection of dtypes or strings to be included/excluded. At least\n",
      "        one of these parameters must be supplied.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        The subset of the frame including the dtypes in ``include`` and\n",
      "        excluding the dtypes in ``exclude``.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        * If both of ``include`` and ``exclude`` are empty\n",
      "        * If ``include`` and ``exclude`` have overlapping elements\n",
      "        * If any kind of string dtype is passed in.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    * To select all *numeric* types, use ``np.number`` or ``'number'``\n",
      "    * To select strings you must use the ``object`` dtype, but note that\n",
      "      this will return *all* object dtype columns\n",
      "    * See the `numpy dtype hierarchy\n",
      "      <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\n",
      "    * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n",
      "      ``'datetime64'``\n",
      "    * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n",
      "      ``'timedelta64'``\n",
      "    * To select Pandas categorical dtypes, use ``'category'``\n",
      "    * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n",
      "      0.20.0) or ``'datetime64[ns, tz]'``\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'a': [1, 2] * 3,\n",
      "    ...                    'b': [True, False] * 3,\n",
      "    ...                    'c': [1.0, 2.0] * 3})\n",
      "    >>> df\n",
      "            a      b  c\n",
      "    0       1   True  1.0\n",
      "    1       2  False  2.0\n",
      "    2       1   True  1.0\n",
      "    3       2  False  2.0\n",
      "    4       1   True  1.0\n",
      "    5       2  False  2.0\n",
      "    \n",
      "    >>> df.select_dtypes(include='bool')\n",
      "       b\n",
      "    0  True\n",
      "    1  False\n",
      "    2  True\n",
      "    3  False\n",
      "    4  True\n",
      "    5  False\n",
      "    \n",
      "    >>> df.select_dtypes(include=['float64'])\n",
      "       c\n",
      "    0  1.0\n",
      "    1  2.0\n",
      "    2  1.0\n",
      "    3  2.0\n",
      "    4  1.0\n",
      "    5  2.0\n",
      "    \n",
      "    >>> df.select_dtypes(exclude=['int'])\n",
      "           b    c\n",
      "    0   True  1.0\n",
      "    1  False  2.0\n",
      "    2   True  1.0\n",
      "    3  False  2.0\n",
      "    4   True  1.0\n",
      "    5  False  2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data.select_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data.select_dtypes(exclude=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         <=50K\n",
       "1         <=50K\n",
       "2         <=50K\n",
       "3         <=50K\n",
       "4         <=50K\n",
       "          ...  \n",
       "32556     <=50K\n",
       "32557      >50K\n",
       "32558     <=50K\n",
       "32559     <=50K\n",
       "32560      >50K\n",
       "Name: income, Length: 32561, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <=50K'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.income[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' >50K'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.income[32560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "32556    0\n",
       "32557    1\n",
       "32558    0\n",
       "32559    0\n",
       "32560    1\n",
       "Name: income, Length: 32561, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_apply = data.income.apply(lambda x:1 if x==' >50K' else 0)\n",
    "y_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data.income)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numerical_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "LS = LinearSVC()\n",
    "LS.fit(x_train,y_train)\n",
    "y_pred = LS.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2434473567303421"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25985663082437277"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "LS = SVC(kernel=\"rbf\")\n",
    "LS.fit(x_train,y_train)\n",
    "y_pred = LS.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               int64\n",
       "fnlwgt            int64\n",
       "education_num     int64\n",
       "capital-gain      int64\n",
       "capital-loss      int64\n",
       "hours-per-week    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4170482564283199"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GB = GaussianNB()\n",
    "GB.fit(x_train,y_train)\n",
    "y_pred = GB.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3379751599548363"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GB = GaussianNB()\n",
    "GB.fit(x_train,y_train)\n",
    "y_pred = GB.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31631863882443934"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "GB = BernoulliNB()\n",
    "GB.fit(x_train,y_train)\n",
    "y_pred = GB.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641357027463652"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ABC = AdaBoostClassifier()\n",
    "ABC.fit(x_train,y_train)\n",
    "y_pred = ABC.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5596412556053811"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5))\n",
    "ABC.fit(x_train,y_train)\n",
    "y_pred = ABC.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5696533682145193"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ABC = GradientBoostingClassifier()\n",
    "ABC.fit(x_train,y_train)\n",
    "y_pred = ABC.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, optional (default='deviance')\n",
      " |      loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, optional (default=0.1)\n",
      " |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int (default=100)\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, optional (default=1.0)\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"friedman_mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"friedman_mse\" for the mean squared error with improvement\n",
      " |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      " |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : integer, optional (default=3)\n",
      " |      maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator or 'zero', optional (default=None)\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default: False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  presort : deprecated, default='deprecated'\n",
      " |      This parameter is deprecated and will be removed in v0.24.\n",
      " |  \n",
      " |      .. deprecated :: 0.22\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, optional (default=0.0)\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : array, shape (n_features,)\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_improvement_ : array, shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : array, shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.ensemble.HistGradientBoostingClassifier,\n",
      " |  sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n",
      " |  AdaBoostClassifier\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : array, shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape [n_samples].\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array, shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array, shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array, shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : generator of array, shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like, shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,) or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, optional\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5775095298602287"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ABC = GradientBoostingClassifier(subsample=0.5)\n",
    "ABC.fit(x_train,y_train)\n",
    "y_pred = ABC.predict(x_test)\n",
    "f1_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
