{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does k-Nearest Neighbors Work\n",
    "- The kNN algorithm is belongs to the family of instance-based, competitive learning and lazy learning algorithms.\n",
    "\n",
    "- Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model.\n",
    "\n",
    "- It is a competitive learning algorithm, because it internally uses competition between model elements (data instances) in order to make a predictive decision. The objective similarity measure between data instances causes each data instance to compete to “win” or be most similar to a given unseen data instance and contribute to a prediction.\n",
    "\n",
    "- Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets.\n",
    "\n",
    "- Finally, kNN is powerful because it does not assume anything about the data, other than a distance measure can be calculated consistently between any two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving K-Nearest Neighbors with Math<br>\n",
    "\n",
    "__k-Nearest Neighbors is a very commonly used algorithm for classification, regression and for imputing missing values__. It works great when you have large amount of classes and a few samples per class.<br>\n",
    "<br>\n",
    "__kNN in one sentence__: is an algorithm that classifies or calculates a regression based on the items in the training set that are 'closer' to each of the testing points.<br>\n",
    "<br>\n",
    "k Parameter - Size of Neighborhood<br>\n",
    " - k represents the amount of neighbors to compare data with. That is why it usually k is an odd number.<br>\n",
    " - the bigger the k, the less 'defined' or more smooth are the neighborhoods.<br>\n",
    "<br>\n",
    "\n",
    "__Distance__ is a key factor in order to determine who is the closest. Distance impacts the size and characteristics of the neighborhoods.  The most commonly used is Euclidean distance since it gives the closest distance between 2 points.<br>\n",
    "<br>\n",
    "Most Common Distances<br>\n",
    " - Euclidean: the shortest distance between to points that might not be the best option when features are normalized. Typically used in face recognition.<br>\n",
    " - Taxicab or Manhattan: is the sum of the absolute differences of the Cartesian coordinates of 2 points. It works the same way as when a car needs to move around 'blocks' to get to the destination.<br>\n",
    " - Minkowski: is a mix of both Euclidean and Manhattan.<br>\n",
    "\n",
    "<br>\n",
    "The amount of features impacts kNN significantly because the more points we have, the more 'unique' each neighborhood becomes. It also affects speed because we need to measure each distance first in order to determine who are the closest k neighbors.<br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our Own KNN from Scratch\n",
    "- A machine learning algorithm usually consists of 2 main blocks:\n",
    "\n",
    "- a training block that takes as input the training data X and the corresponding target y and outputs a learned model.\n",
    "\n",
    "- a predict block that takes as input new and unseen observations and uses the model to output their corresponding responses.\n",
    "\n",
    "In the case of KNN, which as discussed earlier, is a lazy algorithm, the training block reduces to just memorizing the training data. Let’s go ahead a write a python method that does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    # do nothing\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gosh, that was hard! Now we need to write the predict method which must do the following: it needs to compute the euclidean distance between the “new” observation and all the data points in the training set. It must then select the K nearest ones and perform a majority vote. It then assigns the corresponding label to the observation. Let’s go ahead and write that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(x1,y1)(x2,y2)<br>\n",
    "(3,4)(0,0)<br>\n",
    "(3,0)(4,0)<br>\n",
    "(4,5)(1,1)<br>\n",
    "L2 or Eucledian distance:\n",
    "`np.sqrt((x2-x1)**2+(y2-y1)**2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sepal_lenght1,sepal_width1,Petal_length1,petal_width1),<br>(sepal_lenght2,sepal_width2,Petal_length2,petal_width3)<br>\n",
    "(x1,y1,z1,w1),(x2,y2,w2,z2)<br>\n",
    "`np.sqrt((x2-x1)**2 +(y2-y1)**2+(z2-z1)**2+(w2-w1)**2))`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate Euclidean distance\n",
    "def euclidean_distance(x1,x2):\n",
    "    # x1,x2 are numpy arrays\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'euclidean_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bf84decaec83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#output should be 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#(0-1)**2+(1-2)**2 + (2-3)**2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'euclidean_distance' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 =np.array([1,2,3])\n",
    "x2 = np.array([0,1,2])\n",
    "print(euclidean_distance(x1,x2))\n",
    "#output should be 3\n",
    "#(0-1)**2+(1-2)**2 + (2-3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate L1 distance\n",
    "def manhattan_distance(x1,x2):\n",
    "    # x1,x2 are numpy arrays\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([[2.3,4.5,3,4],[5,3,2,1],[4,1,0.5,3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_train, y_train, x_test, k,p):\n",
    "    # create empty list for distances and targets\n",
    "    \n",
    "    \n",
    "    # compute the distances for each datapoint in x_test to all the points in x_train\n",
    "    # Append the distances to distances list\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        # Iterate through all the data points in x_train\n",
    "        for j in range(len(x_train)):\n",
    "            # compute and store distnace based on value for p\n",
    "            # p=1  use L1 distnce,p=2 use L2 distance  \n",
    "            # Note : In the each sublist no of elements should be same as no of rows in x_train\n",
    "            \n",
    "            \n",
    "\n",
    "    # sort each sublist in the disatnces in ascending order (Use np.argsort)\n",
    "    \n",
    "    \n",
    "\n",
    "    # make a list of the k neighbors' targets from the sorted first k indices \n",
    "    \n",
    "    \n",
    "\n",
    "    # return most common target as  alist for each point in x_test\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load_breast_cancer from sklearn datasets\n",
    "- Understnd the data and observe the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your own KNN algorithm and check the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Sklean KNN and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
